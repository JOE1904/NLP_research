{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># <center> **NLP Final Research Paper Project**\n",
    "### **Project Title :** A Document Read-Chat-Note Making Assistant\n",
    "**A joint initiative by: <br>\n",
    "Krish Goyal(21112015) and Joan Job(21112037)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Key Highlights, Features, Steps and Research Gaps to Understand:**\n",
    "##### **Features:** \n",
    "1. The applications supports <u>*.pdf*</u> and <u>*.txt*</u> file formats for document uploads.<br><hr>\n",
    "2. If not, the user has the liberty to copy-paste the required text into the text-box provided. <br><hr>\n",
    "3. The application then scans through the text provided, check for grammatical errors and corrects them <br>\n",
    "    if necessary. (If time permits us to do it.) <br><hr>\n",
    "4. All keywords present in the data will be highlighted/ underlined \n",
    "    (mostly Proper Nouns such as names of places, states, countries, unique personalities etc.)<br><hr>\n",
    "5. There is also an option for the user to chat with the extension for text summaries, word meanings, synonyms etc. <br><hr>\n",
    "6. The Notes file generated can be downloaded seperately as .txt or .pdf file for offline use (as proposed).<br><hr>\n",
    "\n",
    "##### **Basic Architecture Requirements:**  <u></u>\n",
    "1. PDF to Text File Conversion <br> \n",
    "2. Text Preprocessing <br>\n",
    "3. Wikipedia API/Library or Webscraping for extracting text for Named-Entities.\n",
    "4. Text Summarizing Model with the Metrics for Comparative Study for both:\n",
    "    - Named-Entities with and their introductory paragraphs.\n",
    "    - Also for the text document as a whole. \n",
    "    - Summary for Each Subheadings (If Possible.) <br>\n",
    "5. Name-Entity-Recognition(NER) Model with most apt results.<br>\n",
    "6. A Chatbot to chat with User and Understand User Inputs to the Above mentioned Functions.<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "1.\n",
    "1.\n",
    "1.\n",
    "1.\n",
    "1.\n",
    "1.\n",
    "1.\n",
    "1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PDF to Text File Conversion and Cleaning/Formatting(Basic) <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re \n",
    "from datetime import datetime\n",
    "\n",
    "# Function to read PDF files and store the extracted text in a text file\n",
    "def read_pdf(file_path):\n",
    "    timestamp = datetime.now().strftime(\"Y%Y_M%m_D%dT_%H_%M_%S\") \n",
    "    cleaned_path = re.sub(r'[:.\\/\\\\*\\?\"<>|]', '_', file_path)\n",
    "    new_text_file_name =  f\"{cleaned_path}_output_{timestamp}.txt\"\n",
    "    with open(file_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "        text_to_txt = clean_text(text)\n",
    "    try:\n",
    "        with open(new_text_file_name, 'w', encoding='utf-8') as file:\n",
    "            file.write(text_to_txt)\n",
    "        print(f\"Text has been successfully written to {new_text_file_name}\")\n",
    "        return new_text_file_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error while writing to the file: {e}\")\n",
    "\n",
    "\n",
    "# Function to read text files and store the text in a variable\n",
    "def read_text(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as txt_file:\n",
    "            text = txt_file.read()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing the text file: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove line breaks and extra spaces\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "\n",
    "    # Remove unwanted characters\n",
    "    text = re.sub(r'[^A-Za-z0-9.,?!()\\'\":;\\- ]', '', text)\n",
    "    \n",
    "    # Add missing spaces after punctuation marks\n",
    "    text = re.sub(r'([.,?!();:])', r'\\1 ', text)\n",
    "    \n",
    "    # Remove extra spaces after punctuation marks\n",
    "    text = re.sub(r' +([.,?!();:])', r'\\1', text)\n",
    "    \n",
    "    # Remove spaces before punctuation marks\n",
    "    text = re.sub(r' ([.,?!();:])', r'\\1', text)\n",
    "    \n",
    "    # Remove spaces before and after hyphens\n",
    "    text = re.sub(r' - ', ' -', text)\n",
    "    text = re.sub(r' -', '-', text)\n",
    "    text = re.sub(r'- ', '-', text)\n",
    "    return text\n",
    "\n",
    "# Function to identify and process the uploaded file\n",
    "def process_uploaded_file(uploaded_file_path):\n",
    "    if uploaded_file_path.endswith('.pdf'):\n",
    "        # It's a PDF, read and store the text in a text file\n",
    "        file_name = read_pdf(uploaded_file_path)\n",
    "        with open(file_name, 'r', encoding='utf-8') as txt_file:\n",
    "            text = txt_file.readlines()\n",
    "        return text\n",
    "\n",
    "    elif uploaded_file_path.endswith('.txt'):\n",
    "        # It's a text file, read the text directly into a variable\n",
    "        return read_text(uploaded_file_path)\n",
    "    else:\n",
    "        print(\"Unsupported file format. Please upload a PDF or a text file.\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def remove_special_characters(input_string):\n",
    "    # Remove special characters and brackets using regex\n",
    "    cleaned_string = re.sub(r'[^a-zA-Z0-9\\s]', '', input_string)\n",
    "\n",
    "    # Remove accents using Unicode normalization\n",
    "    cleaned_string = unicodedata.normalize('NFKD', cleaned_string).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegasus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A new deep residual network, called Wide ResNet, has been developed.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "\n",
    "ARTICLE_TO_SUMMARIZE = ('''The biggest disadvantage of deep residual networks, according to some, is the feature reuse problem, in which some feature changes or blocks may contribute relatively little to learning. Wide ResNet was formed to solve this issue. The major learning potential of deep residual networks, according to Zagoruyko and Komodakis, is attributable to the residual units, whereas depth has a supplemental influence. ResNet was made wide rather than deep to take use of the residual blocks' strength''')\n",
    "inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook - BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c095be6d6cdc4c9a9f56b8f0c7dcb46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b7bca10c5e4fe185d9e673191cd592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)neration_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1b360b489042a094fcf387a5dabca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e100fd60b1fb4b618ae06a8d7a3b3b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc25241708cc4816945bd80bf1542121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f8c03d38394a55bbecf07d471cc196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e50bc25d9a414595aa4f935c632d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2b01a89d184297867459c0e4a34a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/300 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63ab35536224c0abe1ec129ad907d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10875899f7ab41508e85ce86357dced3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba0f21a6dc24d78b056f9fbd3b04c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"Jeff wants to train a Transformers model on Amazon SageMaker. He can use the new Hugging Face Deep Learning Container. Jeff can find the documentation on Huggingface's blog.    .   The blog is available at: https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugling-face.\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"philschmid/bart-large-cnn-samsum\")\n",
    "\n",
    "conversation = '''Jeff: Can I train a ðŸ¤— Transformers model on Amazon SageMaker? \n",
    "Philipp: Sure you can use the new Hugging Face Deep Learning Container. \n",
    "Jeff: ok.\n",
    "Jeff: and how can I get started? \n",
    "Jeff: where can I find documentation? \n",
    "Philipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face                                           \n",
    "'''\n",
    "summarizer(conversation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67316dfe45954794956abbdb2b80c8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7083952edb514b2a9e7965878e0c0126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ee831b78224ff8afa72431d0d2b028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d13624080bd42c88badd8da425b0ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1ef995216244589fbc8af434c6c5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a007a2055a8a4e1f9e145e0a852059be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effe817896064f98941e909d8a189c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "hf_name = 'pszemraj/led-large-book-summary'\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    hf_name,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "wall_of_text = \"your words here\"\n",
    "\n",
    "result = summarizer(\n",
    "    wall_of_text,\n",
    "    min_length=16,\n",
    "    max_length=256,\n",
    "    no_repeat_ngram_size=3,\n",
    "    encoder_no_repeat_ngram_size=3,\n",
    "    repetition_penalty=3.5,\n",
    "    num_beams=4,\n",
    "    early_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: torch in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping c:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn-0.12.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn-0.12.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn-0.12.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn-0.12.2.dist-info due to invalid metadata entry 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.99)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping c:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn-0.12.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn-0.12.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn-0.12.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn-0.12.2.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_https://huggingface.co/sarakolding/daT5-summariser_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T5ForConditionalGeneration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\COLLEGE DATA\\SEM V\\BDS573BL - Natural Language Processing\\NLP-Project\\project.ipynb Cell 21\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Load pre-trained T5 model and tokenizer:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mt5-large\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m T5ForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m intro_text \u001b[39min\u001b[39;00m df[\u001b[39m\"\u001b[39m\u001b[39mIntro_text\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'T5ForConditionalGeneration' is not defined"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# from transformers import pipeline\n",
    "\n",
    "# # Load pre-trained T5 model and tokenizer:\n",
    "# model_name = \"t5-large\"\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# for intro_text in df[\"Intro_text\"]:\n",
    "#   inputs = tokenizer.encode(\"summarize: \" + text_to_summarize, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "#   summary_ids = model.generate(inputs, max_length=120, min_length=150, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "#   # Decode and print the summary\n",
    "#   summary_tf_small = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "#   df[\"Summary_tf-small\"] = summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/nlp/t5_hf_summarization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830e1d91a9744196bda24380a22e2ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/892 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\goyal\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f483e06122864ebaac48d84b0cc3fb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/977M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8775560d1249dd91289af4273dd3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/419 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1e855b86bd47aca90f319ebcf2ba9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/767k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170c03ff0e344e43b3230cd62a1ef79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/2.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22588a46132141bd930f71122f24f50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=\"sarakolding/daT5-summariser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_https://huggingface.co/aszfcxcgszdx/article-summarizer-t5-large?text=I+love+AutoTrain+%F0%9F%A4%97_   --- See this --- (best model in gaining all the validation scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu111/torch_stable.html\n",
      "Requirement already satisfied: torch in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\goyal\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn-0.12.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn-0.12.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn-0.12.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn-0.12.2.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu111/torch_stable.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\COLLEGE DATA\\SEM V\\BDS573BL - Natural Language Processing\\NLP-Project\\project.ipynb Cell 28\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCuda available: \u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available())\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDevice name:\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mget_device_name())\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Step 2: Check Tensorflow\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m device_lib\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:365\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the name of a device.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \n\u001b[0;32m    356\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m     \u001b[39mreturn\u001b[39;00m get_device_properties(device)\u001b[39m.\u001b[39mname\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:395\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device_properties\u001b[39m(device: _device_t) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    386\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \n\u001b[0;32m    388\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[39m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 395\u001b[0m     _lazy_init()  \u001b[39m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    396\u001b[0m     device \u001b[39m=\u001b[39m _get_device_index(device, optional\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    397\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count():\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Step 1: Check Pytorch (optional)\n",
    "import torch\n",
    "print(\"Cuda available: \", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name())\n",
    "# Step 2: Check Tensorflow\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "# Step 3: Check Keras (optional)\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "my_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32, device=\"cpu\")\n",
    "print(my_tensor)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\COLLEGE DATA\\SEM V\\BDS573BL - Natural Language Processing\\NLP-Project\\project.ipynb Cell 27\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mhumarin/chatgpt_paraphraser_on_T5_base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mhumarin/chatgpt_paraphraser_on_T5_base\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparaphrase\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     question,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     num_beams\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m ):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     input_ids \u001b[39m=\u001b[39m tokenizer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mparaphrase: \u001b[39m\u001b[39m{\u001b[39;00mquestion\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlongest\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X34sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     )\u001b[39m.\u001b[39minput_ids\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2179\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2174\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2175\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2176\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2177\u001b[0m     )\n\u001b[0;32m   2178\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2179\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
    "\n",
    "def paraphrase(\n",
    "    question,\n",
    "    num_beams=5,\n",
    "    num_beam_groups=5,\n",
    "    num_return_sequences=5,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=128\n",
    "):\n",
    "    input_ids = tokenizer(\n",
    "        f'paraphrase: {question}',\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
    "        max_length=max_length, diversity_penalty=diversity_penalty\n",
    "    )\n",
    "\n",
    "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\"\"\"\n",
    "paraphrase(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\COLLEGE DATA\\SEM V\\BDS573BL - Natural Language Processing\\NLP-Project\\project.ipynb Cell 31\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Example code to move a tensor to the GPU\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tensor_on_cpu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tensor_on_gpu \u001b[39m=\u001b[39m tensor_on_cpu\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Example code to move a tensor to the GPU\n",
    "tensor_on_cpu = torch.randn(3, 3)\n",
    "tensor_on_gpu = tensor_on_cpu.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'httpcore' has no attribute 'NetworkBackend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\COLLEGE DATA\\SEM V\\BDS573BL - Natural Language Processing\\NLP-Project\\project.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvalidate_email\u001b[39;00m \u001b[39mimport\u001b[39;00m validate_email\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/COLLEGE%20DATA/SEM%20V/BDS573BL%20-%20Natural%20Language%20Processing/NLP-Project/project.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m is_valid \u001b[39m=\u001b[39m validate_email(email_address\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvt@alliswell.in\u001b[39m\u001b[39m'\u001b[39m, check_regex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, check_mx\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, from_address\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcireta7980@othao.com\u001b[39m\u001b[39m'\u001b[39m, helo_host\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmy.host.name\u001b[39m\u001b[39m'\u001b[39m, smtp_timeout\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, dns_timeout\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, use_blacklist\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, debug\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\validate_email\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvalidate_email\u001b[39;00m \u001b[39mimport\u001b[39;00m validate_email, validate_email_or_fail  \u001b[39m# NOQA\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\validate_email\\validate_email.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mssl\u001b[39;00m \u001b[39mimport\u001b[39;00m SSLContext\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdns_check\u001b[39;00m \u001b[39mimport\u001b[39;00m dns_check\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdomainlist_check\u001b[39;00m \u001b[39mimport\u001b[39;00m domainlist_check\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39memail_address\u001b[39;00m \u001b[39mimport\u001b[39;00m EmailAddress\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\validate_email\\dns_check.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrdatatype\u001b[39;00m \u001b[39mimport\u001b[39;00m MX \u001b[39mas\u001b[39;00m rdtype_mx\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrdtypes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mANY\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mMX\u001b[39;00m \u001b[39mimport\u001b[39;00m MX\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresolver\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     NXDOMAIN, YXDOMAIN, Answer, NoAnswer, NoNameservers, resolve)\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m HOST_REGEX\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39memail_address\u001b[39;00m \u001b[39mimport\u001b[39;00m EmailAddress\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dns\\resolver.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Dict, Iterator, List, Optional, Sequence, Tuple, Union\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparse\u001b[39;00m \u001b[39mimport\u001b[39;00m urlparse\n\u001b[1;32m---> 30\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_ddr\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39medns\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dns\\_ddr.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minet\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mname\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnameserver\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquery\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrdtypes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msvcbbase\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dns\\nameserver.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparse\u001b[39;00m \u001b[39mimport\u001b[39;00m urlparse\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39masyncbackend\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39masyncquery\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minet\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmessage\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dns\\asyncquery.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransaction\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_asyncbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m NullContext\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdns\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquery\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m     BadResponse,\n\u001b[0;32m     40\u001b[0m     NoDOH,\n\u001b[0;32m     41\u001b[0m     NoDOQ,\n\u001b[0;32m     42\u001b[0m     UDPMode,\n\u001b[0;32m     43\u001b[0m     _compute_times,\n\u001b[0;32m     44\u001b[0m     _have_http2,\n\u001b[0;32m     45\u001b[0m     _matches_destination,\n\u001b[0;32m     46\u001b[0m     _remaining,\n\u001b[0;32m     47\u001b[0m     have_doh,\n\u001b[0;32m     48\u001b[0m     ssl,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[39mif\u001b[39;00m have_doh:\n\u001b[0;32m     52\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mhttpx\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goyal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dns\\query.py:67\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhttpcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_backends\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msync\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhttpx\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m _CoreNetworkBackend \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39;49mNetworkBackend\n\u001b[0;32m     68\u001b[0m _CoreSyncStream \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39m_backends\u001b[39m.\u001b[39msync\u001b[39m.\u001b[39mSyncStream\n\u001b[0;32m     70\u001b[0m _have_httpx \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'httpcore' has no attribute 'NetworkBackend'"
     ]
    }
   ],
   "source": [
    "from validate_email import validate_email\n",
    "is_valid = validate_email(email_address='vt@alliswell.in', check_regex=True, check_mx=True, from_address='cireta7980@othao.com', helo_host='my.host.name', smtp_timeout=10, dns_timeout=10, use_blacklist=True, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%load_ext_cudf.pandas` not found.\n"
     ]
    }
   ],
   "source": [
    "%load_ext_cudf.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.env' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"d:/COLLEGE DATA/SEM V/BDS572L - Full Stack Web Development/Week 6/Antique Cafe/.env/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "At the command line, only need to run once to install the package via pip:\n",
    "\n",
    "$ pip install google-generativeai\n",
    "\"\"\"\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyBeKXOpP1-_Uuxl8BseTdR19uvlAnIbGlo\")\n",
    "\n",
    "defaults = {\n",
    "  'model': 'models/text-bison-001',\n",
    "  'temperature': 0.6,\n",
    "  'candidate_count': 1,\n",
    "  'top_k': 40,\n",
    "  'top_p': 0.95,\n",
    "  'max_output_tokens': 1024,\n",
    "  'stop_sequences': [],\n",
    "  'safety_settings': [{\"category\":\"HARM_CATEGORY_DEROGATORY\",\"threshold\":1},{\"category\":\"HARM_CATEGORY_TOXICITY\",\"threshold\":1},{\"category\":\"HARM_CATEGORY_VIOLENCE\",\"threshold\":2},{\"category\":\"HARM_CATEGORY_SEXUAL\",\"threshold\":2},{\"category\":\"HARM_CATEGORY_MEDICAL\",\"threshold\":2},{\"category\":\"HARM_CATEGORY_DANGEROUS\",\"threshold\":2}],\n",
    "}\n",
    "\n",
    "prompt = f\"\"\"Summarize this paragraph and detail some relevant context.\n",
    "\n",
    "Text: \"I am by birth a Genevese, and my family is one of the most distinguished of that republic. My ancestors had been for many years counsellors and syndics, and my father had filled several public situations with honour and reputation. He was respected by all who knew him for his integrity and indefatigable attention to public business. He passed his younger days perpetually occupied by the affairs of his country; a variety of circumstances had prevented his marrying early, nor was it until the decline of life that he became a husband and the father of a family.\"\n",
    "\n",
    "Summary: In this text, the narrator is describing his background and upbringing. He tells us that he is a native of Geneva, Switzerland, and that his family is one of the most distinguished in the republic. He then goes on to describe his father, who was a respected public servant.\n",
    "\n",
    "Text: \"The thing the Time Traveller held in his hand was a glittering metallic framework, scarcely larger than a small clock, and very delicately made. There was ivory in it, and some transparent crystalline substance. And now I must be explicit, for this that followsâ€”unless his explanation is to be acceptedâ€”is an absolutely unaccountable thing. He took one of the small octagonal tables that were scattered about the room, and set it in front of the fire, with two legs on the hearthrug.\"\n",
    "\n",
    "Summary:\"\"\"\n",
    "\n",
    "response = genai.generate_text(\n",
    "  **defaults,\n",
    "  prompt=prompt\n",
    ")\n",
    "\n",
    "print(response.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
